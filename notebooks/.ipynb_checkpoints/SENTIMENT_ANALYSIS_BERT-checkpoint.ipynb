{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting nvidia-ml-py3==7.352.0\n",
      "  Using cached nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pydantic==1.7.4\n",
      "  Using cached pydantic-1.7.4-cp37-cp37m-manylinux2014_x86_64.whl (9.1 MB)\n",
      "Collecting botocore==1.24.42\n",
      "  Using cached botocore-1.24.42-py3-none-any.whl (8.7 MB)\n",
      "Collecting torch==1.8.1\n",
      "  Using cached torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1 MB)\n",
      "Collecting torchtext==0.9.1\n",
      "  Using cached torchtext-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
      "Collecting nltk==3.6.2\n",
      "  Using cached nltk-3.6.2-py3-none-any.whl (1.5 MB)\n",
      "Collecting pandas==1.1.5\n",
      "  Using cached pandas-1.1.5-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n",
      "Collecting scikit-learn==0.24.1\n",
      "  Using cached scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
      "Collecting numpy==1.19.5\n",
      "  Using cached numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "Collecting trax==1.3.7\n",
      "  Using cached trax-1.3.7-py2.py3-none-any.whl (521 kB)\n",
      "Collecting transformers==4.5.1\n",
      "  Using cached transformers-4.5.1-py3-none-any.whl (2.1 MB)\n",
      "Collecting datasets==2.1.0\n",
      "  Using cached datasets-2.1.0-py3-none-any.whl (325 kB)\n",
      "Collecting gsutil==5.5\n",
      "  Using cached gsutil-5.5.tar.gz (2.9 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from botocore==1.24.42->-r ../requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from botocore==1.24.42->-r ../requirements.txt (line 3)) (1.26.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from botocore==1.24.42->-r ../requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from torch==1.8.1->-r ../requirements.txt (line 4)) (4.4.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from torchtext==0.9.1->-r ../requirements.txt (line 5)) (2.28.1)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from torchtext==0.9.1->-r ../requirements.txt (line 5)) (4.64.1)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from nltk==3.6.2->-r ../requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from nltk==3.6.2->-r ../requirements.txt (line 6)) (8.1.3)\n",
      "Requirement already satisfied: regex in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from nltk==3.6.2->-r ../requirements.txt (line 6)) (2022.10.31)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from pandas==1.1.5->-r ../requirements.txt (line 7)) (2022.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from scikit-learn==0.24.1->-r ../requirements.txt (line 8)) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from scikit-learn==0.24.1->-r ../requirements.txt (line 8)) (3.1.0)\n",
      "Collecting tensorflow-datasets\n",
      "  Using cached tensorflow_datasets-4.8.1-py3-none-any.whl (5.2 MB)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from trax==1.3.7->-r ../requirements.txt (line 10)) (1.16.0)\n",
      "Collecting jax\n",
      "  Using cached jax-0.3.25.tar.gz (1.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting funcsigs\n",
      "  Using cached funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\n",
      "Collecting t5\n",
      "  Using cached t5-0.9.3-py3-none-any.whl (153 kB)\n",
      "Collecting gin-config\n",
      "  Using cached gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
      "Collecting tensorflow-text\n",
      "  Using cached tensorflow_text-2.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from trax==1.3.7->-r ../requirements.txt (line 10)) (5.9.3)\n",
      "Collecting gym\n",
      "  Using cached gym-0.26.2.tar.gz (721 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jaxlib\n",
      "  Using cached jaxlib-0.3.25-cp37-cp37m-manylinux2014_x86_64.whl (71.2 MB)\n",
      "Collecting absl-py\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.5.1->-r ../requirements.txt (line 11)) (3.8.0)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Using cached tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.5.1->-r ../requirements.txt (line 11)) (21.3)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.53.tar.gz (880 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.5.1->-r ../requirements.txt (line 11)) (4.11.4)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from datasets==2.1.0->-r ../requirements.txt (line 12)) (0.10.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from datasets==2.1.0->-r ../requirements.txt (line 12)) (2022.10.0)\n",
      "Collecting pyarrow>=5.0.0\n",
      "  Using cached pyarrow-10.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.0 MB)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from datasets==2.1.0->-r ../requirements.txt (line 12)) (0.3.6)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from datasets==2.1.0->-r ../requirements.txt (line 12)) (0.70.14)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from datasets==2.1.0->-r ../requirements.txt (line 12)) (3.8.3)\n",
      "Collecting argcomplete>=1.9.4\n",
      "  Using cached argcomplete-2.0.0-py2.py3-none-any.whl (37 kB)\n",
      "Collecting crcmod>=1.7\n",
      "  Using cached crcmod-1.7.tar.gz (89 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fasteners>=0.14.1\n",
      "  Using cached fasteners-0.18-py3-none-any.whl (18 kB)\n",
      "Collecting gcs-oauth2-boto-plugin>=3.0\n",
      "  Using cached gcs-oauth2-boto-plugin-3.0.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting google-apitools>=0.5.32\n",
      "  Using cached google_apitools-0.5.32-py3-none-any.whl (135 kB)\n",
      "Collecting httplib2>=0.18\n",
      "  Using cached httplib2-0.21.0-py3-none-any.whl (96 kB)\n",
      "Collecting google-reauth>=0.1.0\n",
      "  Using cached google_reauth-0.1.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting monotonic>=1.4\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: pyOpenSSL>=0.13 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from gsutil==5.5->-r ../requirements.txt (line 13)) (22.1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting retry_decorator>=1.0.0\n",
      "  Using cached retry_decorator-1.1.1.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets==2.1.0->-r ../requirements.txt (line 12)) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets==2.1.0->-r ../requirements.txt (line 12)) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets==2.1.0->-r ../requirements.txt (line 12)) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets==2.1.0->-r ../requirements.txt (line 12)) (22.1.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets==2.1.0->-r ../requirements.txt (line 12)) (0.13.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets==2.1.0->-r ../requirements.txt (line 12)) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets==2.1.0->-r ../requirements.txt (line 12)) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets==2.1.0->-r ../requirements.txt (line 12)) (1.8.1)\n",
      "Requirement already satisfied: rsa==4.7.2 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from gcs-oauth2-boto-plugin>=3.0->gsutil==5.5->-r ../requirements.txt (line 13)) (4.7.2)\n",
      "Collecting boto>=2.29.1\n",
      "  Using cached boto-2.49.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting oauth2client>=2.2.0\n",
      "  Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from rsa==4.7.2->gcs-oauth2-boto-plugin>=3.0->gsutil==5.5->-r ../requirements.txt (line 13)) (0.4.8)\n",
      "Collecting pyu2f\n",
      "  Using cached pyu2f-0.1.5.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from httplib2>=0.18->gsutil==5.5->-r ../requirements.txt (line 13)) (3.0.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.1.0->-r ../requirements.txt (line 12)) (5.4.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.1->-r ../requirements.txt (line 11)) (3.10.0)\n",
      "Requirement already satisfied: cryptography<39,>=38.0.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from pyOpenSSL>=0.13->gsutil==5.5->-r ../requirements.txt (line 13)) (38.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from requests->torchtext==0.9.1->-r ../requirements.txt (line 5)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from requests->torchtext==0.9.1->-r ../requirements.txt (line 5)) (2022.9.24)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from gym->trax==1.3.7->-r ../requirements.txt (line 10)) (2.2.0)\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Using cached gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Collecting jax\n",
      "  Using cached jax-0.3.24.tar.gz (1.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.23.tar.gz (1.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.22.tar.gz (1.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.21.tar.gz (1.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.20.tar.gz (1.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.19.tar.gz (1.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.17.tar.gz (1.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.16.tar.gz (1.0 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.15.tar.gz (1.0 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting opt_einsum\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting etils[epath]\n",
      "  Using cached etils-0.9.0-py3-none-any.whl (140 kB)\n",
      "Collecting jaxlib\n",
      "  Using cached jaxlib-0.3.24-cp37-cp37m-manylinux2014_x86_64.whl (70.0 MB)\n",
      "  Using cached jaxlib-0.3.22-cp37-cp37m-manylinux2014_x86_64.whl (72.1 MB)\n",
      "  Using cached jaxlib-0.3.20-cp37-cp37m-manylinux2014_x86_64.whl (72.3 MB)\n",
      "  Using cached jaxlib-0.3.15-cp37-none-manylinux2014_x86_64.whl (72.0 MB)\n",
      "Collecting rouge-score\n",
      "  Using cached rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tfds-nightly\n",
      "  Using cached tfds_nightly-4.8.1.dev202301130045-py3-none-any.whl (5.2 MB)\n",
      "Requirement already satisfied: babel in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from t5->trax==1.3.7->-r ../requirements.txt (line 10)) (2.10.3)\n",
      "Collecting sacrebleu\n",
      "  Using cached sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
      "Collecting mesh-tensorflow[transformer]>=0.1.13\n",
      "  Using cached mesh_tensorflow-0.1.21-py3-none-any.whl (385 kB)\n",
      "Collecting seqio\n",
      "  Using cached seqio-0.0.14-py3-none-any.whl (320 kB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting editdistance\n",
      "  Using cached editdistance-0.6.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
      "Collecting termcolor\n",
      "  Using cached termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from tensorflow-datasets->trax==1.3.7->-r ../requirements.txt (line 10)) (3.20.3)\n",
      "Collecting tensorflow-metadata\n",
      "  Using cached tensorflow_metadata-1.12.0-py3-none-any.whl (52 kB)\n",
      "Collecting dm-tree\n",
      "  Using cached dm_tree-0.1.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (153 kB)\n",
      "Requirement already satisfied: toml in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from tensorflow-datasets->trax==1.3.7->-r ../requirements.txt (line 10)) (0.10.2)\n",
      "Requirement already satisfied: importlib-resources in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from tensorflow-datasets->trax==1.3.7->-r ../requirements.txt (line 10)) (5.10.0)\n",
      "Collecting promise\n",
      "  Using cached promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorflow-hub>=0.8.0\n",
      "  Using cached tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "Collecting tensorflow<2.12,>=2.11.0\n",
      "  Using cached tensorflow-2.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from cryptography<39,>=38.0.0->pyOpenSSL>=0.13->gsutil==5.5->-r ../requirements.txt (line 13)) (1.15.1)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from mesh-tensorflow[transformer]>=0.1.13->t5->trax==1.3.7->-r ../requirements.txt (line 10)) (0.18.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyasn1-modules>=0.0.5\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Using cached tensorboard-2.11.1-py3-none-any.whl (6.0 MB)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text->trax==1.3.7->-r ../requirements.txt (line 10)) (0.2.0)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.7.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Using cached libclang-15.0.6.1-py2.py3-none-manylinux2010_x86_64.whl (21.5 MB)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Using cached keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.51.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m120.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting protobuf>=3.12.2\n",
      "  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Using cached tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text->trax==1.3.7->-r ../requirements.txt (line 10)) (65.5.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.29.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m118.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Using cached flatbuffers-23.1.4-py2.py3-none-any.whl (26 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Using cached wrapt-1.14.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n",
      "INFO: pip is looking at multiple versions of pyyaml to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "INFO: pip is looking at multiple versions of pyparsing to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "INFO: pip is looking at multiple versions of protobuf to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting protobuf>=3.12.2\n",
      "  Using cached protobuf-4.21.12-cp37-abi3-manylinux2014_x86_64.whl (409 kB)\n",
      "INFO: pip is looking at multiple versions of oauth2client to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting oauth2client>=2.2.0\n",
      "  Using cached oauth2client-4.1.2-py2.py3-none-any.whl (99 kB)\n",
      "INFO: pip is looking at multiple versions of multidict to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
      "INFO: pip is looking at multiple versions of mesh-tensorflow[transformer] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting mesh-tensorflow[transformer]>=0.1.13\n",
      "  Using cached mesh_tensorflow-0.1.20-py3-none-any.whl (385 kB)\n",
      "INFO: pip is looking at multiple versions of idna to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "INFO: pip is looking at multiple versions of gym-notices to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Using cached gym_notices-0.0.7-py3-none-any.whl (2.7 kB)\n",
      "INFO: pip is looking at multiple versions of frozenlist to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.3.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
      "INFO: pip is looking at multiple versions of etils[enp,epath] to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of cryptography to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting cryptography<39,>=38.0.0\n",
      "  Using cached cryptography-38.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
      "INFO: pip is looking at multiple versions of cloudpickle to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting cloudpickle>=1.2.0\n",
      "  Using cached cloudpickle-2.2.0-py3-none-any.whl (25 kB)\n",
      "INFO: pip is looking at multiple versions of charset-normalizer to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting charset-normalizer<3.0,>=2.0\n",
      "  Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "INFO: pip is looking at multiple versions of certifi to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "INFO: pip is looking at multiple versions of boto to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting boto>=2.29.1\n",
      "  Using cached boto-2.48.0-py2.py3-none-any.whl (1.4 MB)\n",
      "INFO: pip is looking at multiple versions of attrs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting attrs>=17.3.0\n",
      "  Using cached attrs-22.2.0-py3-none-any.whl (60 kB)\n",
      "INFO: pip is looking at multiple versions of async-timeout to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "INFO: pip is looking at multiple versions of etils[enp,epath] to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of aiosignal to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "INFO: pip is looking at multiple versions of xxhash to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "INFO: pip is looking at multiple versions of tensorflow-text to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow-text\n",
      "  Using cached tensorflow_text-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "Collecting tensorflow<2.11,>=2.10.0\n",
      "  Using cached tensorflow-2.10.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n",
      "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
      "  Using cached tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
      "Collecting keras<2.11,>=2.10.0\n",
      "  Using cached keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting tensorboard<2.11,>=2.10\n",
      "  Using cached tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "Collecting tensorflow<2.11,>=2.10.0\n",
      "  Using cached tensorflow-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-text\n",
      "  Using cached tensorflow_text-2.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "Collecting tensorflow<2.10,>=2.9.0\n",
      "  Using cached tensorflow-2.9.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\n",
      "  Using cached keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Using cached tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Using cached tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "Collecting tensorflow<2.10,>=2.9.0\n",
      "  Using cached tensorflow-2.9.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\n",
      "  Using cached tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
      "  Using cached tensorflow-2.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
      "Collecting tensorflow-text\n",
      "  Using cached tensorflow_text-2.8.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting tensorflow<2.9,>=2.8.0\n",
      "  Using cached tensorflow-2.8.4-cp37-cp37m-manylinux2010_x86_64.whl (497.9 MB)\n",
      "Collecting gast>=0.2.1\n",
      "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting tensorflow-estimator<2.9,>=2.8\n",
      "  Downloading tensorflow_estimator-2.8.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 kB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.9,>=2.8.0rc0\n",
      "  Using cached keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting tensorflow<2.9,>=2.8.0\n",
      "  Using cached tensorflow-2.8.3-cp37-cp37m-manylinux2010_x86_64.whl (497.9 MB)\n",
      "  Using cached tensorflow-2.8.2-cp37-cp37m-manylinux2010_x86_64.whl (497.9 MB)\n",
      "  Using cached tensorflow-2.8.1-cp37-cp37m-manylinux2010_x86_64.whl (497.9 MB)\n",
      "  Downloading tensorflow-2.8.0-cp37-cp37m-manylinux2010_x86_64.whl (497.5 MB)\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m462.6/497.5 MB\u001b[0m \u001b[31m206.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Upgrade dependencies\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading the dataset\n",
    "\n",
    "We will use the __pandas__ library to read our dataset. Let's first download the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Training data:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train_complete.csv', encoding='utf-8', header=0)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Test data:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open('../data/test_text.txt') as f:\n",
    "    lines = [line.rstrip('\\n') for line in f]\n",
    "print(len(lines))\n",
    "test_df = pd.DataFrame (lines, columns = ['tweet'])\n",
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if there is imbalance\n",
    "class_id_distribution = train_df['label'].value_counts()\n",
    "print(class_id_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22215, 2)\n"
     ]
    }
   ],
   "source": [
    "# Improving the imbalance\n",
    "train_df_sample_class_0 = train_df[train_df['label'] == 0]\n",
    "train_df_sample_class_1 = train_df[train_df['label'] == 1][:7405]\n",
    "train_df_sample_class_2 = train_df[train_df['label'] == 2][:7405]\n",
    "\n",
    "train_df = pd.concat([train_df_sample_class_0, train_df_sample_class_1, train_df_sample_class_2])\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(subset=[\"tweet\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "from transformers import Trainer, TrainingArguments, DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This separates 10% of the entire train dataset into validation dataset.\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df[\"tweet\"].tolist(),\n",
    "    train_df[\"label\"].tolist(),\n",
    "    test_size=0.20,\n",
    "    shuffle=True,\n",
    "    random_state=324,\n",
    "    stratify = train_df[\"label\"].tolist(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(train_texts,\n",
    "                            truncation=True,\n",
    "                            padding=True)\n",
    "val_encodings = tokenizer(val_texts,\n",
    "                          truncation=True,\n",
    "                          padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "class ReviewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]).to(device) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx]).to(device)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "train_dataset = ReviewDataset(train_encodings, train_labels)\n",
    "val_dataset = ReviewDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\",\n",
    "                                                            num_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train_loss 0.9334. Val_loss 0.9351.     Val_accuracy 0.5543. Seconds 57.234.\n",
      "Epoch 1. Train_loss 0.8554. Val_loss 0.8085.     Val_accuracy 0.6322. Seconds 59.803.\n",
      "Epoch 2. Train_loss 0.8350. Val_loss 0.8524.     Val_accuracy 0.6090. Seconds 61.521.\n",
      "Epoch 3. Train_loss 0.8202. Val_loss 0.8494.     Val_accuracy 0.6029. Seconds 60.794.\n",
      "Epoch 4. Train_loss 0.8117. Val_loss 0.8115.     Val_accuracy 0.6297. Seconds 60.295.\n",
      "Epoch 5. Train_loss 0.8052. Val_loss 0.7864.     Val_accuracy 0.6432. Seconds 61.372.\n",
      "Epoch 6. Train_loss 0.8048. Val_loss 0.8276.     Val_accuracy 0.6068. Seconds 60.846.\n",
      "Epoch 7. Train_loss 0.7952. Val_loss 0.8104.     Val_accuracy 0.6347. Seconds 60.551.\n",
      "Epoch 8. Train_loss 0.7921. Val_loss 0.8099.     Val_accuracy 0.6187. Seconds 60.417.\n",
      "Epoch 9. Train_loss 0.7886. Val_loss 0.8017.     Val_accuracy 0.6245. Seconds 60.210.\n"
     ]
    }
   ],
   "source": [
    "# Implement this\n",
    "# Freeze the encoder weights until the classfier\n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate=0.02\n",
    "\n",
    "# Get the compute device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8, drop_last=True)\n",
    "eval_dataloader = DataLoader(val_dataset, batch_size=8, drop_last=True)\n",
    "\n",
    "# Setup the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "metric = load_metric(\"f1\")\n",
    "\n",
    "model=model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    training_loss = 0\n",
    "    val_loss = 0\n",
    "    # Training loop starts\n",
    "    model.train() # put the model in training mode\n",
    "    for batch in train_dataloader:\n",
    "        # below: ** allows us to pass multiple arguments to model()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        training_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # Validation loop starts\n",
    "    model.eval() # put the model in prediction mode\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            # below:  ** allows us to pass multiple arguments to model()\n",
    "            outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        val_loss += loss.item()\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        \n",
    "    # Let's take the average losses\n",
    "    training_loss = training_loss / len(train_dataloader)\n",
    "    val_loss = val_loss / len(eval_dataloader)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(f\"Epoch {epoch}. Train_loss {training_loss:.4f}. Val_loss {val_loss:.4f}. \\\n",
    "    Val_accuracy {metric.compute()['accuracy']:.4f}. Seconds {end-start:.3f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the val predictions\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4)\n",
    "val_predictions = []\n",
    "model.eval()\n",
    "for batch in val_dataloader:\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    val_predictions.extend(predictions.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the train predictions\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4)\n",
    "train_predictions = []\n",
    "model.eval()\n",
    "for batch in train_dataloader:\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    train_predictions.extend(predictions.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 986  201  294]\n",
      " [ 286  634  561]\n",
      " [ 119  207 1155]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.67      0.69      1481\n",
      "           1       0.61      0.43      0.50      1481\n",
      "           2       0.57      0.78      0.66      1481\n",
      "\n",
      "    accuracy                           0.62      4443\n",
      "   macro avg       0.63      0.62      0.62      4443\n",
      "weighted avg       0.63      0.62      0.62      4443\n",
      "\n",
      "Accuracy (validation): 0.62457798784605\n"
     ]
    }
   ],
   "source": [
    "# Check the confusion matrix of validation dataset\n",
    "y_val = val_labels\n",
    "# y_val = y_val.detach().cpu().numpy()\n",
    "print(confusion_matrix(y_val, val_predictions))\n",
    "print(classification_report(y_val, val_predictions))\n",
    "print(\"Accuracy (validation):\", accuracy_score(y_val, val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4221  659 1044]\n",
      " [1143 2588 2193]\n",
      " [ 429  680 4815]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.71      0.72      5924\n",
      "           1       0.66      0.44      0.53      5924\n",
      "           2       0.60      0.81      0.69      5924\n",
      "\n",
      "    accuracy                           0.65     17772\n",
      "   macro avg       0.66      0.65      0.64     17772\n",
      "weighted avg       0.66      0.65      0.64     17772\n",
      "\n",
      "Accuracy (training): 0.654062570335359\n"
     ]
    }
   ],
   "source": [
    "# Check the confusion matrix of train dataset\n",
    "y_train = train_labels\n",
    "# y_val = y_val.detach().cpu().numpy()\n",
    "print(confusion_matrix(y_train, train_predictions))\n",
    "print(classification_report(y_train, train_predictions))\n",
    "print(\"Accuracy (training):\", accuracy_score(y_train, train_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make predictions on your test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12032, 1)\n"
     ]
    }
   ],
   "source": [
    "# Implement this\n",
    "print(test_df.shape)\n",
    "test_df.isna().sum()\n",
    "test_df[\"tweet\"] = test_df[\"tweet\"].fillna(value='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = test_df[\"tweet\"].tolist()\n",
    "test_encodings = tokenizer(test_texts,\n",
    "                          truncation=True,\n",
    "                          padding=True)\n",
    "test_dataset = ReviewDataset(test_encodings, [0]*len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=4)\n",
    "test_predictions = []\n",
    "model.eval()\n",
    "for batch in test_dataloader:\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    test_predictions.extend(predictions.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12032\n"
     ]
    }
   ],
   "source": [
    "print(len(test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write your predictions to a CSV file\n",
    "You can use the following code to write your test predictions to a CSV file. Then upload your file to https://mlu.corp.amazon.com/contests/redirect/53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame()\n",
    "result_df[\"tweet\"] = test_df[\"tweet\"]\n",
    "result_df[\"label\"] = test_predictions\n",
    " \n",
    "result_df.to_csv(\"../../data/open_data_sentiment_analysis/result.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
